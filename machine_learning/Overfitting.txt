Overfitting:-

When the model performs exceptionally well on train dataset and very poor on test dataset, then we can say that the model is suffering from 
overfitting. Overfitting can be high bias towards the trained dataset but doesn't generalize well to real world dataset. 

In order to solve the problem of overfitting we use different techniques like,

1. Regularization (L1 and L2)
2. dropout
3. Data Augmentation
4. Early Stopping

1. Regularization

Regularization is a technique which makes slight modifications to the learning algorithm such that the model generalizes better. This in turn 
improves the model’s performance on the unseen data as well.

We have studied the concept of regularization in machine learning, that the regularization penalizes the coefficients but in deep learning, 
it actually penalizes the weight matrices of the nodes.

Assume that our regularization coefficient is so high that some of the weight matrices are nearly equal to zero. This will result in a much 
simpler linear network and slight underfitting of the training data. Such a large value of the regularization coefficient is not that useful.
We need to optimize the value of regularization coefficient in order to obtain a well-fitted model as shown in the image below.

Different Regularization Techniques 

L1 and L2:- L1 and L2 are the most common types of regularization. These update the general cost function by adding another term known as the 
regularization term. Due to the addition of this regularization term, the values of weight matrices decrease because it assumes that a neural 
network with smaller weight matrices leads to simpler models. Therefore, it will also reduce overfitting to quite an extent. However, this 
regularization term differs in L1 and L2.  L2 regularization is also known as weight decay as it forces the weights to decay towards zero 
(but not exactly zero).


2. dropout

This is the one of the most interesting types of regularization techniques. It also produces very good results and is consequently the most 
frequently used regularization technique in the field of deep learning. At every iteration, it randomly selects some nodes and removes them 
along with all of their incoming and outgoing connections as shown below. This probability of choosing how many nodes should be dropped is 
the hyperparameter of the dropout function.


3. Data Augmentation

The simplest way to reduce overfitting is to increase the size of the training data. In machine learning, we were not able to increase the 
size of training data as the labeled data was too costly. But, now let’s consider we are dealing with images. In this case, there are a few 
ways of increasing the size of the training data – rotating the image, flipping, scaling, shifting, etc. In the below image, some 
transformation has been done on the handwritten digits dataset.

4. Early Stopping

Early stopping is a kind of cross-validation strategy where we keep one part of the training set as the validation set. When we see that the 
performance on the validation set is getting worse, we immediately stop the training on the model. This is known as early stopping.



Reference:-
1) https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/